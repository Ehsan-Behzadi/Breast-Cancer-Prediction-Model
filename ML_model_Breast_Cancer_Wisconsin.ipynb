{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of Contents</h1>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "    <ol>\n",
    "        <li><a href=\"#import_libraries\">Import Libraries</a></li>\n",
    "        <li><a href=\"#import_dataset\">Import \"Breast Cancer Wisconsin (Original)\" Dataset</a></li>\n",
    "        <li><a href=\"#information\">Information about the Dataset</a></li>\n",
    "        <li><a href=\"#pre-processing\">Pre-processing</a></li>        \n",
    "        <li><a href=\"#feature_selection\">Feature Selection</a></li>\n",
    "        <li><a href=\"#classification\">Classification</a></li>        \n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"import_libraries\"> \n",
    "    <h2>Import Libraries</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.feature_selection import RFE \n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, roc_auc_score \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"import_dataset\"> \n",
    "    <h2>Import \"Breast Cancer Wisconsin (Original)\" Dataset</h2>         \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About the dataset:**  \n",
    "<ul>  \n",
    "    <li>  \n",
    "        The \"<strong>Breast Cancer Wisconsin (Original) Dataset</strong>\" contains <strong>699 samples</strong> of breast cancer cases. Each sample includes the following <strong>10 features</strong>:  \n",
    "        <ul>  \n",
    "            <li><strong>ID number:</strong> An identifier for each sample.</li>  \n",
    "            <li><strong>Clump Thickness:</strong> A measurement of the thickness of the clump of cells.</li>  \n",
    "            <li><strong>Uniformity of Cell Size:</strong> Measures how uniform the size of the cells is.</li>  \n",
    "            <li><strong>Uniformity of Cell Shape:</strong> Assesses how uniform the shape of the cells is.</li>  \n",
    "            <li><strong>Marginal Adhesion:</strong> The adhesion of the cells at the margins.</li>  \n",
    "            <li><strong>Single Epithelial Cell Size:</strong> The size of a single epithelial cell.</li>  \n",
    "            <li><strong>Bare Nuclei:</strong> The presence of nuclei without surrounding cytoplasm.</li>  \n",
    "            <li><strong>Bland Chromatin:</strong> The texture of the chromatin in the cell nucleus.</li>  \n",
    "            <li><strong>Normal Nucleoli:</strong> The presence of normal nucleoli in the cells.</li>  \n",
    "            <li><strong>Mitoses:</strong> The count of cells undergoing mitosis.</li>  \n",
    "        </ul>  \n",
    "        Additionally, the dataset includes a <strong>Class</strong> label indicating whether the tumor is <strong>benign (2)</strong> or <strong>malignant (4)</strong>.  \n",
    "        <br><br>  \n",
    "    </li>  \n",
    "    <li>  \n",
    "        This is a well-known dataset used for research in medical care and machine learning. Researchers and practitioners commonly use this dataset for:  \n",
    "        <ol>  \n",
    "            <li><strong>Classification Tasks:</strong> Predicting whether a breast mass is benign or malignant using machine learning algorithms.</li>  \n",
    "            <li><strong>Feature Selection:</strong> Identifying the most relevant features for classification.</li>  \n",
    "            <li><strong>Model Evaluation:</strong> Comparing the performance of different machine learning models on a standardized dataset.</li>  \n",
    "            <li><strong>Educational Purposes:</strong> Teaching students and practitioners about data preprocessing, feature extraction, and model building in the context of machine learning.</li>  \n",
    "        </ol>  \n",
    "        <br>  \n",
    "    </li>  \n",
    "    <li>  \n",
    "        Researchers have used this dataset to achieve various findings, including:  \n",
    "        <ol>  \n",
    "            <li><strong>Improved Classification Accuracy:</strong> Developing and refining machine learning models to enhance the accuracy of breast cancer diagnosis.</li>  \n",
    "            <li><strong>Feature Importance:</strong> Identifying which features are most significant for distinguishing between benign and malignant masses.</li>  \n",
    "            <li><strong>Model Comparisons:</strong> Comparing the performance of different algorithms (e.g., Decision Trees, Support Vector Machines, Neural Networks) to find the most effective approach for this task.</li>  \n",
    "            <li><strong>Data Augmentation:</strong> Exploring techniques to augment the dataset and improve model performance.</li>  \n",
    "        </ol>        \n",
    "    </li>  \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\" \n",
    "column_names = [\"id number\" , \"Clump Thickness\" , \"Uniformity of Cell Size\" , \"Uniformity of Cell Shape\" , \"Marginal Adhesion\" ,\n",
    "                \"Single Epithelial Cell Size\" , \"Bare Nuclei\" , \"Bland Chromatin\" , \"Normal Nucleoli\" , \"Mitoses\" , \"Class\"]  \n",
    "bcw_df = pd.read_csv(url, header=None, names=column_names)\n",
    "display(bcw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"information\"> \n",
    "    <h2>Information about the Dataset</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show summary statistics for the dataset\n",
    "# This includes count, mean, standard deviation, minimum, 25%, 50%, 75%, and maximum values for numeric columns\n",
    "print('\\nThe dataset description:\\n')\n",
    "\n",
    "data_describe = bcw_df.describe()\n",
    "display(data_describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a concise summary of the dataset\n",
    "# This summary includes the index dtype, column dtypes, non-null values, and memory usage \n",
    "print('\\nMore information about the dataset:\\n')\n",
    "\n",
    "data_information = bcw_df.info()\n",
    "display(data_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the dataset, which returns the number of rows and columns\n",
    "shape_of_the_dataset = bcw_df.shape\n",
    "print(\"\\nThe shape of the dataset -->\", shape_of_the_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of unique values in each column of the dataset\n",
    "print('\\nNumber of unique data in the dataset:\\n')\n",
    "\n",
    "unique_data = bcw_df.nunique()\n",
    "print(unique_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"pre-processing\"> \n",
    "    <h2>Pre-processing</h2>    \n",
    "</div>\n",
    "<div>\n",
    "    <ol>\n",
    "        <li><a href=\"#duplicates\">Duplicate Tuples</a></li>\n",
    "        <li><a href=\"#outliers\">Detecting Outliers (Noise)</a></li>\n",
    "        <li><a href=\"#missing_values\">Handling Missing Values</a></li>\n",
    "        <li><a href=\"#standardization\">Standardization</a></li>     \n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"duplicates\"> \n",
    "    <h2>Duplicate Tuples</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting 'id number' column because it has no effect on learning process\n",
    "bcw_df = bcw_df.drop('id number', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of duplicate rows in the dataframe\n",
    "Num_of_duplicate_rows = bcw_df.duplicated().sum()\n",
    "print(\"\\nThe number of duplicate rows -->\", Num_of_duplicate_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all duplicated rows in the dataframe  \n",
    "# 'duplicated(keep=False)' marks all duplicates (including the first occurrence as True)\n",
    "df_all_duplicate = bcw_df[bcw_df.duplicated(keep=False)]\n",
    "print(\"\\nAll the rows and their duplicates:\\n\")\n",
    "display(df_all_duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify only the duplicated rows in the dataframe\n",
    "# 'duplicated()' without any parameters, meaning its output only shows the rows that are duplicates and excludes the first occurrences\n",
    "duplicate = bcw_df[bcw_df.duplicated()]\n",
    "print(\"\\nJust duplicate rows:\\n\")\n",
    "display(duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all duplicate rows from the dataframe\n",
    "# df_ADD --> df_after dropping duplicates\n",
    "df_ADD = bcw_df.drop_duplicates()\n",
    "print(\"\\nThe dataset after dropping the duplicate tuples:\\n\")\n",
    "display(df_ADD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"outliers\"> \n",
    "    <h2>Detecting Outliers (Noise)</h2>    \n",
    "</div>\n",
    "<div>\n",
    "    <ol>\n",
    "        <li><a href=\"#iqr\">Interquartile Range (IQR) method</a></li> \n",
    "        <li><a href=\"#db_scan\">DBSCAN Clustering (Density-Based Spatial Clustering)</a></li> \n",
    "        <li><a href=\"#output\">Output the results</a></li>        \n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"iqr\"> \n",
    "    <h2>Interquartile Range (IQR) method</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns from the dataframe\n",
    "# Ignore the 'Bare Nuclei' column from the original dataframe, because its type is object \n",
    "numeric_df = df_ADD.select_dtypes(include=['number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Q1 (25th percentile) and Q3 (75th percentile)  \n",
    "Q1 = numeric_df.quantile(0.25)  \n",
    "Q3 = numeric_df.quantile(0.75)  \n",
    "IQR = Q3 - Q1  \n",
    "\n",
    "# Define the outlier detection bounds  \n",
    "lower_bound = Q1 - 1.5 * IQR  \n",
    "upper_bound = Q3 + 1.5 * IQR  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask to filter out rows with outliers  \n",
    "outlier_mask = ~((numeric_df < lower_bound) |   \n",
    "                 (numeric_df > upper_bound)).any(axis=1)  \n",
    "\n",
    "# Create a new dataframe after outlier detection and deleting \n",
    "df_iqr = numeric_df[outlier_mask]  \n",
    "display(df_iqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the IQR method\n",
    "# Separate features and target variable  \n",
    "x = df_iqr.drop('Class', axis=1)            # Features\n",
    "y = df_iqr['Class']                         # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80/20) \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN classifier  \n",
    "clf_iqr = KNeighborsClassifier(n_neighbors=1)  \n",
    "\n",
    "# Perform cross-validation to check accuracy after IQR outlier removal  \n",
    "accuracy_iqr = np.mean(cross_val_score(clf_iqr, x_train, y_train, scoring='accuracy', cv=10))  \n",
    "print(f'\\nCross-validated accuracy after IQR outlier removal: {accuracy_iqr:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the 'Bare Nuclei' column from the original dataframe \n",
    "df_iqr['Bare Nuclei'] = df_ADD['Bare Nuclei']\n",
    "\n",
    "# Specify the desired column order\n",
    "columns_order = [\"Clump Thickness\", \"Uniformity of Cell Size\", \"Uniformity of Cell Shape\", \"Marginal Adhesion\",\n",
    "                 \"Single Epithelial Cell Size\", \"Bare Nuclei\", \"Bland Chromatin\", \"Normal Nucleoli\", \"Mitoses\", \"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the dataframe columns\n",
    "df_iqr = df_iqr[columns_order].reset_index(drop=True)\n",
    "\n",
    "# Display updated dataframe\n",
    "print(\"Updated dataframe:\") \n",
    "display(df_iqr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"db_scan\"> \n",
    "    <h2>DBSCAN Clustering (Density-Based Spatial Clustering)</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns from the dataframe, ignoring the 'Bare Nuclei' column (datatype: object)  \n",
    "numeric_df = df_ADD.select_dtypes(include=['number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features (z-score normalization) \n",
    "scaler = StandardScaler() \n",
    "X_scaled = scaler.fit_transform(numeric_df.drop(columns=[\"Class\"]))\n",
    "\n",
    "# Set the parameters for DBSCAN: eps and min_samples\n",
    "dbs = DBSCAN(eps=2, min_samples=5).fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels of the clusters(outliers will have the label -1) \n",
    "labels = dbs.labels_\n",
    "\n",
    "# Add cluster labels to the dataframe \n",
    "numeric_df['Cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the outliers (rows with label -1) \n",
    "df_dbs = numeric_df[numeric_df['Cluster'] != -1]\n",
    "\n",
    "# Display the dataframe without outliers \n",
    "print(\"\\nDataframe after removing outliers:\\n\") \n",
    "display(df_dbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of detected outliers \n",
    "n_outliers = (labels == -1).sum() \n",
    "print(f'\\nNumber of outliers detected: {n_outliers}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of clusters, ignoring noise if present \n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0) \n",
    "print(f'\\nEstimated number of clusters: {n_clusters}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the DBSCAN method\n",
    "# Separate features and target variable  \n",
    "x = df_dbs.drop('Class', axis=1)            # Features\n",
    "y = df_dbs['Class']                         # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80/20) \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN classifier  \n",
    "clf_dbs = KNeighborsClassifier(n_neighbors=1)  \n",
    "\n",
    "# Perform cross-validation to check accuracy after DBSCAN outlier removal  \n",
    "accuracy_dbs = np.mean(cross_val_score(clf_dbs, x_train, y_train, scoring='accuracy', cv=10))  \n",
    "print(f'\\nCross-validated accuracy after DBSCAN outlier removal: {accuracy_dbs:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'Cluster' column to clean up the dataframe\n",
    "df_dbs = df_dbs.drop(columns=['Cluster'])\n",
    "\n",
    "# Add the 'Bare Nuclei' column from the original dataframe \n",
    "df_dbs['Bare Nuclei'] = df_ADD['Bare Nuclei']\n",
    "\n",
    "# Specify the desired column order for the final dataframe\n",
    "columns_order = [\"Clump Thickness\", \"Uniformity of Cell Size\", \"Uniformity of Cell Shape\", \"Marginal Adhesion\",\n",
    "                 \"Single Epithelial Cell Size\", \"Bare Nuclei\", \"Bland Chromatin\", \"Normal Nucleoli\", \"Mitoses\", \"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the dataframe columns\n",
    "df_dbs = df_dbs[columns_order].reset_index(drop=True)\n",
    "\n",
    "# Display updated dataframe\n",
    "print(\"\\nUpdated dataframe:\\n\") \n",
    "display(df_dbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"output\"> \n",
    "    <h2>Output the results</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the results of different outlier detection methods   \n",
    "print('\\nIQR result:', accuracy_iqr)                   # Print accuracy score for the iqr method  \n",
    "print('\\nDBSCAN clustering result:', accuracy_dbs)     # Print accuracy score for the dbscan clustering method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nContinue working with DBSCAN Clustering after comparing different outlier detection methods:\\n\")\n",
    "display(df_dbs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the dataset\n",
    "print(\"\\nDataset shape before dropping duplicate tuples -->\", bcw_df.shape)\n",
    "print(\"\\nDataset shape after dropping duplicate tuples -->\", df_ADD.shape)\n",
    "print(\"\\nDataset shape after deleting the outliers -->\", df_dbs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"missing_values\"> \n",
    "    <h2>Handling Missing Values</h2>    \n",
    "</div>\n",
    "<div>\n",
    "    <ol>\n",
    "        <li><a href=\"#drop\">Drop Imputation</a></li>\n",
    "        <li><a href=\"#median\">Median Imputation</a></li>        \n",
    "        <li><a href=\"#iterative\">Iterative Imputation</a></li>        \n",
    "        <li><a href=\"#output\">Output the results</a></li>    \n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a concise summary of the dataset after deleting the outliers\n",
    "# This summary includes the index dtype, column dtypes, non-null values, and memory usage \n",
    "print('\\nMore information about the dataset after deleting the outliers:\\n')\n",
    "\n",
    "data_information = df_dbs.info()\n",
    "display(data_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the 'Bare Nuclei' column\n",
    "df_dbs['Bare Nuclei'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataframe\n",
    "isna = pd.DataFrame(df_dbs.isna().sum(axis=0))\n",
    "print(isna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nThere are no NaN values in the dataset')\n",
    "print('But according to the description, it has the missing values are in other shapes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find missing values ​​in other shapes\n",
    "# Define unwanted values and consider them as null/missing  \n",
    "unwanted_values = ['?', '!', '$', 'None', 'null', '']\n",
    "\n",
    "# Replace unwanted values with NaN   \n",
    "df_dbs.replace(unwanted_values, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any NaN values now present in the dataframe  \n",
    "missing_values_count = df_dbs.isna().sum() \n",
    "\n",
    "# Display the count of missing values for each column  \n",
    "print(\"\\nCount of missing values in each column:\")  \n",
    "print(missing_values_count[missing_values_count > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display rows with missing values  \n",
    "rows_with_missing = df_dbs[df_dbs.isna().any(axis=1)]  \n",
    "print(\"\\nRows with missing values:\")  \n",
    "display(rows_with_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Bare Nuclei' column to float\n",
    "df_dbs['Bare Nuclei'] = df_dbs['Bare Nuclei'].astype(\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"drop\"> \n",
    "    <h2>Drop Imputation</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original dataset \n",
    "DfDrop = df_dbs.copy(deep=True)\n",
    "\n",
    "# Fill missing values in 'Bare Nuclei' with '0'\n",
    "DfDrop['Bare Nuclei'] = DfDrop['Bare Nuclei'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'Bare Nuclei' is 0 \n",
    "DfDrop.drop(DfDrop.index[(DfDrop[\"Bare Nuclei\"] == 0)],axis=0,inplace=True)\n",
    "\n",
    "# Preview the data after drop imputation  \n",
    "print('\\nPreview the data after drop imputation: \\n')\n",
    "display(DfDrop.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"median\"> \n",
    "    <h2>Median Imputation</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original dataset   \n",
    "DfDrop_med = df_dbs.copy(deep=True)\n",
    "\n",
    "# Fill missing values in 'Bare Nuclei'  \n",
    "# Fill with the median of the column\n",
    "median_bare_nuclei = DfDrop_med['Bare Nuclei'].median()\n",
    "DfDrop_med['Bare Nuclei'].fillna(median_bare_nuclei, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data after median imputation  \n",
    "print('\\nPreview the data after median imputation: \\n')\n",
    "display(DfDrop_med.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"iterative\"> \n",
    "    <h2>Iterative Imputation</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original dataset  \n",
    "DfIterative = df_dbs.copy(deep=True)  \n",
    "\n",
    "# Keep only numeric columns for iterative imputation\n",
    "df_ite_numeric = DfIterative.select_dtypes(include=[np.number])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the iterative imputer  \n",
    "imputer_ite = IterativeImputer(missing_values=np.nan, sample_posterior=True, min_value=0, \n",
    "                               random_state=0)  \n",
    "\n",
    "# Performe the iterative imputation  \n",
    "imputed_data_ite = imputer_ite.fit_transform(df_ite_numeric)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to dataframe  \n",
    "DfIterative[df_ite_numeric.columns] = imputed_data_ite  \n",
    "\n",
    "# Preview the data after iterative imputation  \n",
    "print('\\nPreview the data after iterative imputation:\\n')\n",
    "display(DfIterative.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"output\"> \n",
    "    <h2>Output the results</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the different Imputation Methods using **Kernel Density Estimation (KDE) Plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Bare Nuclei' column \n",
    "# Setup the plotting environment  \n",
    "plt.figure(figsize=(14, 10))  \n",
    "\n",
    "# KDE for 'Bare Nuclei' column  \n",
    "sns.kdeplot(df_dbs['Bare Nuclei'], label='Baseline', fill=False, bw_adjust=0.5)\n",
    "sns.kdeplot(DfDrop['Bare Nuclei'], label='Drop Imputation', fill=False, bw_adjust=0.5) \n",
    "sns.kdeplot(DfDrop_med['Bare Nuclei'], label='Median Imputation', fill=False, bw_adjust=0.5)   \n",
    "sns.kdeplot(DfIterative['Bare Nuclei'], label='Iterative Imputation', fill=False, bw_adjust=0.5) \n",
    "\n",
    "# Aesthetic aspects of the plot  \n",
    "plt.title('KDE Plot comparison of Bare Nuclei across Imputation Methods')  \n",
    "plt.xlabel('Bare Nuclei')  \n",
    "plt.ylabel('Density')  \n",
    "plt.legend()  \n",
    "plt.grid(True)  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nContinue working with drop imputation after comparing different imputation methods:\\n\")\n",
    "display(DfDrop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"standardization\"> \n",
    "    <h2>Standardization</h2>    \n",
    "</div>\n",
    "<div>\n",
    "    <ol>\n",
    "        <li><a href=\"#z-score\">Z-Score Standardization (Standard Scaling)</a></li>\n",
    "        <li><a href=\"#min-max\">Min-Max Scaling (Normalization)</a></li>         \n",
    "        <li><a href=\"#output\">Output the results</a></li>     \n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"z-score\"> \n",
    "    <h2>Z-Score Standardization (Standard Scaling)</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Z-score standardization\n",
    "Z_scaler = StandardScaler()  \n",
    "Z_Scaled = Z_scaler.fit_transform(DfDrop)\n",
    "\n",
    "# Create a new dataframe with the scaled data  \n",
    "df_Z_Scaled = pd.DataFrame(Z_Scaled, columns = list(DfDrop.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all columns except 'Class'\n",
    "df_Z_Scaled_final = df_Z_Scaled.drop('Class', axis = 1)\n",
    "\n",
    "# Add the 'Class' column back to the dataframe\n",
    "df_Z_Scaled_final['Class'] = DfDrop['Class'].tolist()\n",
    "display(df_Z_Scaled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data manipulation in the 'Class' column\n",
    "df_Z_Scaled_final['Class'] = df_Z_Scaled_final['Class'].replace({2: 0, 4: 1}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the Z-score standardization\n",
    "# Separate features and target variable  \n",
    "x_z = df_Z_Scaled_final.drop('Class', axis = 1)               # Features\n",
    "y_z = df_Z_Scaled_final['Class']                              # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80/20)  \n",
    "x_train_z, x_test_z, y_train_z, y_test_z = train_test_split(x_z, y_z, test_size=0.2, random_state=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN classifier  \n",
    "clf_z = KNeighborsClassifier(n_neighbors=10)  \n",
    "\n",
    "# Perform cross-validation to check accuracy after the Z-standard scaling  \n",
    "accuracy_z = np.mean(cross_val_score(clf_z, x_train_z, y_train_z, scoring='accuracy', cv=10)) \n",
    "print(f'\\nCross-validated accuracy after the Z-standard scaling: {accuracy_z:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"min-max\"> \n",
    "    <h2>Min-Max Scaling (Normalization)</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Min Max scaler\n",
    "MM_scaler = MinMaxScaler()\n",
    "Min_Max_Scaled = MM_scaler.fit_transform(DfDrop)\n",
    "\n",
    "# Create a new dataframe with the scaled data \n",
    "df_Min_Max_Scaled_final = pd.DataFrame(Min_Max_Scaled, columns = list(DfDrop.columns))\n",
    "display(df_Min_Max_Scaled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the Min Max Scaler\n",
    "# Separate features and target variable  \n",
    "x_mm = df_Min_Max_Scaled_final.drop('Class', axis = 1)            # Features\n",
    "y_mm = df_Min_Max_Scaled_final['Class']                           # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80/20) \n",
    "x_train_mm, x_test_mm, y_train_mm, y_test_mm = train_test_split(x_mm, y_mm, test_size=0.2, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN classifier  \n",
    "clf_mm = KNeighborsClassifier(n_neighbors=10)  \n",
    "\n",
    "# Perform cross-validation to check accuracy after the Min Max scaling  \n",
    "accuracy_mm = np.mean(cross_val_score(clf_mm, x_train_mm, y_train_mm, scoring='accuracy', cv=10))\n",
    "print(f'\\nCross-validated accuracy after the Min Max scaling: {accuracy_mm:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"output\"> \n",
    "    <h2>Output the results</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the results of different standardization methods   \n",
    "print('\\nZ-standard scaling result:', accuracy_z)  # Print accuracy score for the z-score standardization method  \n",
    "print('\\nMin Max scaling result:', accuracy_mm)    # Print accuracy score for the min max scaler method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nContinue working with the dataset scaled by Z-standard scaling after comparing different scaling methods:\\n\")\n",
    "df_Scaled = df_Z_Scaled_final\n",
    "display(df_Scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"feature_selection\"> \n",
    "    <h2>Feature Selection</h2>    \n",
    "</div>\n",
    "<div>\n",
    "    <ol>\n",
    "        <li><a href=\"#fm\">Filter Method (Correlation Analysis)</a></li>\n",
    "        <li><a href=\"#rfe\">Recursive Feature Elimination (RFE)</a></li>        \n",
    "        <li><a href=\"#mi\">Mutual Information</a></li>       \n",
    "        <li><a href=\"#output\">Output the results</a></li> \t\t\n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"fm\"> \n",
    "    <h2>Filter Method (Correlation Analysis)</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "corr = df_Scaled.corr()\n",
    "print('\\nCorrelation between the features in the dataset:\\n')\n",
    "\n",
    "# Display the correlation matrix\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  \n",
    "    display(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the correlation matrix using a heatmap plot\n",
    "# Setup the plotting environment \n",
    "plt.figure(figsize=(15,15))\n",
    "print('\\nVisualizing the correlation of the dataset:\\n')\n",
    "\n",
    "# Heatmap plot for correlation\n",
    "sns.heatmap(corr, cbar=True, square= True, fmt='.2f', annot=True, annot_kws={'size':10}, cmap='Blues')\n",
    "\n",
    "# Aesthetic aspects of the plot\n",
    "plt.title('Feature Correlation Heatmap', fontsize=18)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and print correlation of 'Class' with other features\n",
    "print(\"\\nThe correlation of 'Class' with other features:\\n\")\n",
    "class_corr = df_Scaled.corr()['Class'].sort_values(ascending=False) \n",
    "print(class_corr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features with correlation >= 0.6 with 'Class'\n",
    "# Apply abs() to consider both positive and negative correlations\n",
    "significant_features_fm = class_corr[class_corr.abs() >= 0.6].index.tolist()    \n",
    "\n",
    "# Remove 'Class' from the list of significant features\n",
    "significant_features_fm = [feature for feature in significant_features_fm if feature != 'Class']            \n",
    "print(\"\\nChoosing features that have correlation >= 0.6':\\n\", significant_features_fm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate filter method\n",
    "# Separate features and target variable\n",
    "x_fm = df_Scaled[significant_features_fm].drop(columns=['Class'], errors='ignore')        # Features \n",
    "y_fm = df_Scaled['Class']                                                                 # Target variable\n",
    "\n",
    "# Split the dataset into training and testing sets (80/20) with random state  \n",
    "X_train, X_test, y_train, y_test = train_test_split(x_fm, y_fm, test_size=0.2, random_state=0, stratify=y_fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN classifier\n",
    "clf_fm = KNeighborsClassifier(n_neighbors=1)  \n",
    "\n",
    "# Perform cross-validation to check accuracy after filter method \n",
    "accuracy_fm = np.mean(cross_val_score(clf_fm, x_fm, y_fm, scoring='accuracy', cv=10))  \n",
    "print(f\"\\nCross-validated accuracy after filter method: {accuracy_fm:.4f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"rfe\"> \n",
    "    <h2>Recursive Feature Elimination (RFE)</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable  \n",
    "X = df_Scaled.drop('Class', axis=1)              # Features  \n",
    "y = df_Scaled['Class']                           # Target variable\n",
    "\n",
    "# Split the dataset into training and testing sets (80/20) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Logistic Regression model  \n",
    "reg_model = LogisticRegression(max_iter=1000)                   # Added max_iter for convergence if needed  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit RFE  \n",
    "rfe = RFE(estimator=reg_model, n_features_to_select=6)          # Select 6 features  \n",
    "\n",
    "# Fit the model to the training data\n",
    "rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the selected features  \n",
    "significant_features_rfe = X.columns[rfe.support_]  \n",
    "print(\"Selected features using RFE:\")  \n",
    "print(significant_features_rfe.tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate RFE\n",
    "# Separate features and target variable\n",
    "X_rfe = df_Scaled[significant_features_rfe]            # Features\n",
    "y = df_Scaled['Class']                                 # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN classifier\n",
    "clf_rfe = KNeighborsClassifier(n_neighbors = 1)\n",
    "\n",
    "# Perform cross-validation to check accuracy after RFE\n",
    "accuracy_rfe = np.mean(cross_val_score(clf_rfe, X_rfe, y, scoring='accuracy', cv=10))\n",
    "print(f\"\\nCross-validated accuracy after RFE: {accuracy_rfe:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"mi\"> \n",
    "\t<h2>Mutual Information</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable  \n",
    "X = df_Scaled.drop('Class', axis=1)              # Features  \n",
    "y = df_Scaled['Class']                           # Target variable\n",
    "\n",
    "# Split the dataset into training and testing sets (80/20) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mutual information  \n",
    "mi = mutual_info_classif(X_train, y_train, discrete_features='auto', random_state=0)  \n",
    "\n",
    "# Create a dataframe to view feature importances  \n",
    "mi_df = pd.DataFrame({'Feature': X.columns, 'Mutual Information': mi}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe based on mutual information  \n",
    "mi_df = mi_df.sort_values(by='Mutual Information', ascending=False)\n",
    "\n",
    "# Display mutual information for each feature  \n",
    "print(\"Mutual information for each feature:\")  \n",
    "print(mi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the mutual information using a bar plot \n",
    "# Setup the plotting environment  \n",
    "plt.figure(figsize=(10, 6))  \n",
    "\n",
    "# Horizontal bar plot for the mutual information\n",
    "plt.barh(mi_df['Feature'], mi_df['Mutual Information'], color='skyblue')  \n",
    "\n",
    "# Aesthetic aspects of the plot\n",
    "plt.xlabel('Mutual Information')  \n",
    "plt.title('Mutual Information')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a fixed threshold \n",
    "fixed_threshold = 0.2\n",
    "\n",
    "# Using fixed threshold  \n",
    "significant_features_mi = mi_df[mi_df['Mutual Information'] > fixed_threshold]\n",
    "print(\"Selected features:\")  \n",
    "print(significant_features_mi) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate MI\n",
    "# Get feature names as a list  \n",
    "selected_feature_names = significant_features_mi['Feature'].tolist()         \n",
    "\n",
    "# Separate features and target variable\n",
    "X_selected_MI = df_Scaled[selected_feature_names]              # Features\n",
    "y = df_Scaled['Class']                                         # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN classifier\n",
    "clf_mi = KNeighborsClassifier(n_neighbors = 1)\n",
    "\n",
    "# Perform cross-validation to check accuracy after mutual information\n",
    "accuracy_mi = np.mean(cross_val_score(clf_mi, X_selected_MI, y, scoring='accuracy', cv=10))\n",
    "print(f\"\\nCross-validated accuracy after mutual information: {accuracy_mi:.4f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"output\"> \n",
    "    <h2>Output the results</h2>    \n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the results of different feature selection methods   \n",
    "print('\\nFilter method result:', accuracy_fm)     # Print accuracy score for filter (correlation analysis) method  \n",
    "print('\\nRFE result:', accuracy_rfe)              # Print accuracy score for recursive feature elimination (RFE) method\n",
    "print('\\nMI result:', accuracy_mi)                # Print accuracy score for mutual information method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nUsing features that obtained from recursive feature elimination (RFE) because it had better accuracy \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final dataset** after feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataset after feature selection (recursive feature elimination (RFE))\n",
    "# Extract the names of the selected features \n",
    "selected_features = significant_features_rfe.tolist()\n",
    "print('\\nselected features:\\n', selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final dataframe with the selected features and add the target column\n",
    "df_final = df_Scaled[selected_features] \n",
    "df_final['Class'] = df_Scaled['Class']\n",
    "\n",
    "# Display the final dataframe\n",
    "print(\"Final dataframe with selected features and target column:\") \n",
    "display(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the 'Class' variable\n",
    "class_counts = df_final['Class'].value_counts()  \n",
    "print(\"Class distribution:\\n\", class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"classification\"> \n",
    "    <h2>Classification</h2>    \n",
    "</div>\n",
    "<div>\n",
    "    <ol>\n",
    "        <li><a href=\"#knn\">K-Nearest Neighbors (KNN)</a></li>   \n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable  \n",
    "X = df_final.drop('Class', axis=1)              # Features  \n",
    "y = df_final['Class']                           # Target variable\n",
    "\n",
    "# Split the dataset into training and testing sets (80/20) \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nThe shape of the X_train dataset -->', X_train.shape)\n",
    "print('\\nThe shape of the Y_train dataset -->', Y_train.shape)\n",
    "print('\\nThe shape of the X_test dataset -->', X_test.shape)\n",
    "print('\\nThe shape of the Y_test dataset -->', Y_test.shape)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"knn\">   \n",
    "    <h2>K-Nearest Neighbors (KNN)</h2>    \n",
    "</div>  \n",
    "<div>  \n",
    "    <ol>  \n",
    "        <li>  \n",
    "            <a href=\"#valid\">Validating</a>  \n",
    "            <ol>   \n",
    "                <li><a href=\"#holdout\">Holdout</a></li>   \n",
    "                <li><a href=\"#rrs\">Repeated Random Sampling</a></li>                \n",
    "            </ol>  \n",
    "        </li>  \n",
    "        <li><a href=\"#test\">Testing</a></li>  \n",
    "        <li><a href=\"#roc\">ROC plot and AUC score</a></li> \n",
    "        <li><a href=\"#output\">Output the results</a></li> \n",
    "    </ol>  \n",
    "</div>  \n",
    "<br>  \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"holdout\"> \n",
    "    <h2>Holdout</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holdout \n",
    "# Split the dataset into training and validating sets (80/20)\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.2)\n",
    "\n",
    "# Train a KNN classifier (with K=5)\n",
    "clf_knn_h = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf_knn_h.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels for the validating data\n",
    "y_predict = clf_knn_h.predict(x_val)\n",
    "\n",
    "# Evaluate model performance\n",
    "print('\\nHoldout result:')\n",
    "accuracy_score_holdout = accuracy_score(y_val, y_predict)\n",
    "print('\\nAccuracy  -->', accuracy_score_holdout)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"rrs\"> \n",
    "    <h2>Repeated Random Sampling</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeated random sampling\n",
    "\n",
    "Accuracy = []             # Initialize a list to store accuracy results\n",
    "num_repeats = 10          # Number of times to repeat random sampling\n",
    "\n",
    "# Perform repeated random sampling\n",
    "for i in range(num_repeats):\n",
    "\n",
    "    # Split the dataset into training and validating sets (80/20)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.2)\n",
    "\n",
    "    # Train a KNN classifier (with K=5)\n",
    "    clf_knn_rrs = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "    # Fit the model to the training data\n",
    "    clf_knn_rrs.fit(x_train, y_train)\n",
    "\n",
    "    # Predict the labels for the validating data\n",
    "    y_val_predict = clf_knn_rrs.predict(x_val)\n",
    "    accuracy_score(y_val, y_val_predict)\n",
    "    Accuracy.append(accuracy_score(y_val, y_val_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "df_Accuracy = pd.DataFrame(Accuracy, columns=['Accuracy'])\n",
    "print('\\nAccuracy in 10 iterations for different train and validation sets:\\n')\n",
    "display(df_Accuracy)\n",
    "\n",
    "accuracy_score_rrs = df_Accuracy.Accuracy.mean()\n",
    "print('\\nThe mean of different accuracies for validating the model -->', accuracy_score_rrs)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"test\"> \n",
    "    <h2>Testing</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "# Train a KNN classifier (with K=5)\n",
    "clf_knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "# Fit the model to the training data\n",
    "clf_knn.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the labels for the testing data\n",
    "Y_predict = clf_knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "print('\\nTesting the model:\\n')\n",
    "\n",
    "accuracy_score_knn_testing = accuracy_score(Y_test, Y_predict)\n",
    "print('\\nAccuracy  -->', accuracy_score_knn_testing)\n",
    "print('\\nRecall or Sensitivity or TPR --->', recall_score(Y_test, Y_predict))\n",
    "print('\\nPrecision -->', precision_score(Y_test, Y_predict))\n",
    "print('\\nF1_score -->', f1_score(Y_test, Y_predict))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display the classification report\n",
    "print('\\nClassification report:\\n', classification_report(Y_test, Y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and display the confusion matrix\n",
    "confusion_matrix = metrics.confusion_matrix(Y_predict, Y_test)\n",
    "\n",
    "# Create a dataframe for the confusion matrix for better visualization\n",
    "confusion_matrix_dataframe = pd.DataFrame(confusion_matrix, columns = ['benign present', 'malignant present'], \n",
    "                                          index = ['test benign', 'test malignant'])\n",
    "print(\"\\nConfusion matrix:\\n\")\n",
    "display(confusion_matrix_dataframe)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.1.6 ROC plot and AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC\n",
    "def plot_roc_curve(y_test, y_prid):\n",
    "\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prid)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC plot and AUC score\n",
    "plot_roc_curve(Y_test, Y_predict)\n",
    "\n",
    "# Calculate AUC score\n",
    "auc_score = roc_auc_score(Y_test, Y_predict)   \n",
    "print('\\nAUC score:', auc_score)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"output\"> \n",
    "    <h2>Output the results</h2>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the results of different validation methods and the Naive Bayes testing  \n",
    "print('\\nHoldout result:', accuracy_score_holdout)                       # Print accuracy score for the holdout method  \n",
    "print('\\nRepeated random sampling result:', accuracy_score_rrs)          # Print accuracy score for repeated random sampling method \n",
    "print('\\nKNN testing result:', accuracy_score_knn_testing)               # Print accuracy score for KNN testing  \n",
    "print('\\nAUC score:', auc_score)                                         # Print AUC score for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
